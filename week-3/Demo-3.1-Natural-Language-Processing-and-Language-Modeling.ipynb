{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WENhPRb9avUV"
      },
      "source": [
        "# 3.1 Natural Langauge Processing and Language Modeling\n",
        "\n",
        "## Attribution\n",
        "This notebook was re-used and modified from material created by NVIDIA and Dartmouth College and licensed under the Creative Commons Attribution-Non Commercial 4.0 International License (CC BY-NC 4.0) for the **Generative AI: Theory and Applications** MSc Module at UWS.\n",
        "Source materials available at: https://developer.nvidia.com/gen-ai-teaching-kit-syllabus (NVIDIA Deep Learning Institute Generative AI Teaching Kit) \n",
        "\n",
        "## Overview\n",
        "\n",
        "Welcome to the first notebook in the third week. In this notebook we will look further into some of tha Natural Language Processing (NLP) tasks and methods that we have seen so far.\n",
        "In particular, by the end of this notebook you will:\n",
        "\n",
        "- Process documents using Bag-of-Words models\n",
        "- Use an N-Gram model to select the most likely next word\n",
        "- Use Named-Entity Recognition to identify key terms in a text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqzcHHDabSwW"
      },
      "outputs": [],
      "source": [
        "# PLEASE UNCOMMENT AND RUN THE FOLLOWING LINE IF YOU ARE USING A COLAB NOTEBOOK\n",
        "# !pip install -qqq nltk scikit-learn transformers torch spacy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qefcM98vbeVk",
        "outputId": "f03580e1-7218-4a5b-d767-a3966c5f592e"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "82dEcPkAcL_W",
        "outputId": "710fd514-fb40-4b15-f747-ec0d11826488"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Javascript\n",
        "display(Javascript('IPython.notebook.kernel.restart();'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5ojb7q6b7fG"
      },
      "source": [
        "## 1. Bag-of-Words (BOW) Representation:\n",
        "**Bag of Words (BoW) Explanation**\n",
        "\n",
        "What is Bag of Words?\n",
        "- The Bag of Words (BoW) model is a simple and widely used technique in natural language processing (NLP) to represent text data.\n",
        "- It converts text into numerical features by counting the occurrences of each word in a document, ignoring grammar, order, or context.\n",
        "- The output is often a sparse matrix where:\n",
        "  - Each row corresponds to a document.\n",
        "  - Each column corresponds to a unique word in the corpus.\n",
        "  - The value represents the frequency (or binary presence) of the word in the document.\n",
        "\n",
        "How Does Bag of Words Work?\n",
        "1. **Tokenization**: Split the text into individual words or tokens.\n",
        "2. **Vocabulary Creation**: Compile a list of unique words across all documents.\n",
        "3. **Vectorization**: Represent each document as a vector of word frequencies or binary indicators.\n",
        "\n",
        "Usefulness of Bag of Words\n",
        "- **Simplicity**: Easy to implement and understand, making it a good starting point for text representation.\n",
        "- **Feature Engineering**: Provides a straightforward way to generate features for machine learning models.\n",
        "- **Compatibility**: Works well with traditional machine learning algorithms like Naive Bayes, Logistic Regression, and SVM.\n",
        "- **Baseline Model**: Serves as a benchmark for evaluating more sophisticated NLP models.\n",
        "\n",
        "Limitations of Bag of Words\n",
        "1. **Loss of Context**:\n",
        "   - Ignores word order and semantic relationships between words.\n",
        "   - Example: \"I love dogs\" and \"Dogs love me\" are treated as identical.\n",
        "2. **High Dimensionality**:\n",
        "   - For large vocabularies, the feature space becomes huge, leading to sparse data and increased computational cost.\n",
        "3. **No Weighting**:\n",
        "   - Frequent but less important words (e.g., \"the\", \"is\") can dominate the representation.\n",
        "   - Mitigated by using term frequency-inverse document frequency (TF-IDF).\n",
        "4. **Poor Generalization**:\n",
        "   - Unseen words in new data are not represented, leading to issues in handling out-of-vocabulary (OOV) words.\n",
        "\n",
        "When to Use Bag of Words?\n",
        "- For small to medium-sized datasets where simplicity and speed are prioritized.\n",
        "- When context and semantic meaning are less critical for the task.\n",
        "- As a baseline to compare against more advanced models like Word2Vec, GloVe, or transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIPWwxzmcyCz",
        "outputId": "0d6dc06e-a1fd-46c0-fe75-4857a7fecb49"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import random\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# For Bag-of-Words classification\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Ensure NLTK data is downloaded (particularly 'punkt' for tokenization)\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "\"\"\"\n",
        "1. BAG-OF-WORDS EXAMPLE:\n",
        "    - Convert text documents into simple frequency vectors.\n",
        "    - Train a naive Bayes classifier on a tiny spam vs. not_spam dataset.\n",
        "    - Evaluate the classifier accuracy on a hold-out test set.\n",
        "\"\"\"\n",
        "print(\"=== Bag-of-Words Example ===\")\n",
        "\n",
        "# Step 1: Create a small dataset of documents and labels\n",
        "# Each document is a short text, and each label indicates whether it is \"spam\" or \"not_spam\"\n",
        "documents = [\n",
        "    \"Win big prizes now\",                         # spam\n",
        "    \"Congratulations you won free tickets\",       # spam\n",
        "    \"Meeting at the office tomorrow\",             # not spam\n",
        "    \"Let's schedule a call about the project\",    # not spam\n",
        "    \"Click here to claim your prize!\",            # spam\n",
        "    \"Weekly report is due by Friday\"              # not spam\n",
        "]\n",
        "labels = [\"spam\", \"spam\", \"not_spam\", \"not_spam\", \"spam\", \"not_spam\"]\n",
        "\n",
        "# Step 2: Convert text data into Bag-of-Words (BOW) feature vectors\n",
        "# The CountVectorizer converts each document into a vector of word counts\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# The vocabulary is the list of unique words across all documents\n",
        "# Each word is assigned a column in the feature matrix\n",
        "print(\"Extracted Vocabulary:\", vectorizer.vocabulary_)  # Print the mapping of words to indices\n",
        "print(\"BOW Feature Matrix Shape:\", X.shape)  # Shape = (#documents, #unique_words)\n",
        "\n",
        "# Step 3: Split the dataset into training and test sets\n",
        "# 67% of the data will be used for training, and 33% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels,\n",
        "                                                    test_size=0.33,\n",
        "                                                    random_state=42)\n",
        "\n",
        "# Step 4: Train a Naive Bayes classifier\n",
        "# Naive Bayes is a simple algorithm that works well with BOW representations\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)  # Train the model on the training set\n",
        "\n",
        "# Step 5: Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate the classifier\n",
        "# Calculate accuracy by comparing predictions to ground truth labels\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy on our tiny dataset: {accuracy:.2f}\")\n",
        "print(\"Test Predictions:\", y_pred)  # Predicted labels for the test set\n",
        "print(\"Test Ground Truth:\", y_test)  # Actual labels for the test set\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLp66G1jdB3F"
      },
      "source": [
        "Let's try again but with a much larger dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikfqNEVndEqO",
        "outputId": "e47b0930-bd7d-4165-a945-e3f204f32aa6"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# Templates for more varied spam and not_spam messages\n",
        "spam_templates = [\n",
        "    \"Win a {prize} by clicking here now!\",\n",
        "    \"Congratulations! You are eligible for a free {item}.\",\n",
        "    \"Claim your exclusive offer for {discount}% off!\",\n",
        "    \"Don't miss out on this limited-time deal!\",\n",
        "    \"You have won {amount} dollars! Act fast to redeem.\",\n",
        "    \"Click to receive your special bonus reward.\"\n",
        "]\n",
        "\n",
        "not_spam_templates = [\n",
        "    \"The meeting is scheduled for {day}.\",\n",
        "    \"Let's discuss the {topic} in our next call.\",\n",
        "    \"Reminder: {task} is due by {deadline}.\",\n",
        "    \"Please review the {document} and provide feedback.\",\n",
        "    \"Join us for the upcoming {event} this {day}.\",\n",
        "    \"Updates on the {project} will be shared soon.\"\n",
        "]\n",
        "\n",
        "# Parameters for generating varied messages\n",
        "spam_placeholders = {\n",
        "    \"prize\": [\"car\", \"trip to Paris\", \"iPhone\", \"gift card\"],\n",
        "    \"item\": [\"laptop\", \"TV\", \"smartphone\", \"headphones\"],\n",
        "    \"discount\": [20, 30, 50, 70],\n",
        "    \"amount\": [1000, 5000, 10000, 50000]\n",
        "}\n",
        "\n",
        "not_spam_placeholders = {\n",
        "    \"day\": [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"],\n",
        "    \"topic\": [\"budget\", \"new project\", \"strategy plan\", \"team goals\"],\n",
        "    \"task\": [\"report\", \"presentation\", \"proposal\", \"draft\"],\n",
        "    \"deadline\": [\"next week\", \"Friday\", \"end of the month\"],\n",
        "    \"document\": [\"report\", \"proposal\", \"presentation slides\", \"spreadsheet\"],\n",
        "    \"event\": [\"workshop\", \"seminar\", \"webinar\", \"meeting\"],\n",
        "    \"project\": [\"marketing campaign\", \"software development\", \"product launch\"]\n",
        "}\n",
        "\n",
        "# Function to populate templates with random placeholders\n",
        "def generate_messages(templates, placeholders, count):\n",
        "    messages = []\n",
        "    for _ in range(count):\n",
        "        template = random.choice(templates)\n",
        "        message = template.format(**{k: random.choice(v) for k, v in placeholders.items()})\n",
        "        messages.append(message)\n",
        "    return messages\n",
        "\n",
        "\n",
        "count = 10\n",
        "# Generate half as much spam as not_spam messages\n",
        "spam_messages = generate_messages(spam_templates, spam_placeholders, count//2)\n",
        "not_spam_messages = generate_messages(not_spam_templates, not_spam_placeholders, count)\n",
        "\n",
        "# Combine and shuffle the dataset\n",
        "documents = spam_messages + not_spam_messages\n",
        "labels = [\"spam\"] * (count//2) + [\"not_spam\"] * count\n",
        "\n",
        "\n",
        "\n",
        "# 1) Convert text data to BOW features\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "print(\"Extracted Vocabulary:\", vectorizer.vocabulary_)  # Show some of the words\n",
        "print(\"BOW Feature Matrix Shape:\", X.shape)\n",
        "\n",
        "# 2) Train a simple classifier (Naive Bayes)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels,\n",
        "                                                    test_size=0.33,\n",
        "                                                    random_state=42)\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 3) Evaluate\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy on our tiny dataset: {accuracy:.2f}\")\n",
        "print(\"Test Predictions:\", y_pred)\n",
        "print(\"Test Ground Truth:\", y_test)\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJch2EUOeNcm"
      },
      "source": [
        "Try playing around with how many spam emails total, and what the proportion is of not spam. Here we are using half as many, think if that is realistic and what effect that has."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf2xu4H0AQGz"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfocEc-jc23o"
      },
      "source": [
        "## 2. N-Gram Language Model:\n",
        "\n",
        "What is an N-Gram Model?\n",
        "- An N-Gram model is an extension of the Bag-of-Words (BoW) model that captures sequences of N consecutive words (or tokens) in the text.\n",
        "- Instead of treating individual words as independent features, N-Grams represent combinations of words to preserve some context.\n",
        "- For example:\n",
        "  - For a text: \"I love machine learning\", the N-Grams with N=2 (bigrams) are:\n",
        "    - [\"I love\", \"love machine\", \"machine learning\"]\n",
        "  - For N=3 (trigrams), the output would be:\n",
        "    - [\"I love machine\", \"love machine learning\"]\n",
        "\n",
        "Why Use N-Grams?\n",
        "- **Preserving Context**: Unlike BoW, N-Grams capture the relationship between words, providing limited context.\n",
        "- **Improved Performance**: Useful in tasks like sentiment analysis, where the sequence of words matters (e.g., \"not good\" vs. \"very good\").\n",
        "- **Better Features**: Helps in distinguishing phrases and expressions that cannot be inferred from individual words alone.\n",
        "\n",
        "How Does an N-Gram Model Work?\n",
        "1. **Tokenization**: Break the text into individual tokens.\n",
        "2. **N-Gram Generation**: Combine N consecutive tokens into phrases.\n",
        "3. **Vocabulary Creation**: Create a list of unique N-Grams across the corpus.\n",
        "4. **Vectorization**: Represent each document as a vector of N-Gram frequencies or binary indicators.\n",
        "\n",
        "Usefulness of N-Gram Models\n",
        "- **Contextual Representation**: Captures short-term dependencies between words, useful in many NLP tasks.\n",
        "- **Flexibility**: Can be applied with varying values of N to balance between word independence (N=1) and full dependency modeling (higher N).\n",
        "- **Baseline Model**: Offers a more context-aware representation than BoW, serving as a bridge to more advanced models.\n",
        "\n",
        "Limitations of N-Gram Models\n",
        "1. **Data Sparsity**:\n",
        "   - Larger N values exponentially increase the number of possible N-Grams, making the feature space sparse.\n",
        "   - Requires a lot of data to avoid overfitting.\n",
        "2. **Loss of Long-Range Context**:\n",
        "   - Captures only local relationships between words; cannot model long-range dependencies.\n",
        "   - Example: \"The dog barked\" vs. \"The dog that lived next door barked.\"\n",
        "3. **High Computational Cost**:\n",
        "   - Larger N-Grams lead to high dimensionality, increasing storage and computation requirements.\n",
        "4. **Vocabulary Explosion**:\n",
        "   - The number of unique N-Grams grows rapidly with the size of the corpus and the value of N.\n",
        "\n",
        "When to Use N-Gram Models?\n",
        "- For tasks where word sequences and short-term context are important.\n",
        "- When working with small to medium datasets where advanced models like transformers are impractical.\n",
        "- As a baseline to compare against more sophisticated context-aware models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUYJpLifezRr",
        "outputId": "dd9de28f-83f5-46f5-9e9b-f0545e58539f"
      },
      "outputs": [],
      "source": [
        "  import nltk\n",
        "  nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UJ2bbibeaGB",
        "outputId": "4d6e6259-5a8a-4581-f3c6-4f2812189e92"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict, Counter\n",
        "import random\n",
        "\n",
        "\"\"\"\n",
        "2. N-GRAM LANGUAGE MODEL EXAMPLE:\n",
        "    - We'll implement a basic bigram model: P(word_n | word_n-1).\n",
        "    - We'll train it on a small corpus and then generate random text.\n",
        "\"\"\"\n",
        "print(\"=== N-Gram Language Model Example ===\")\n",
        "\n",
        "# Step 1: Create a small corpus\n",
        "# This corpus contains a few sentences about NLP and machine learning.\n",
        "corpus = \"\"\"\n",
        "I love natural language processing.\n",
        "I love machine learning.\n",
        "I enjoy writing code for NLP tasks.\n",
        "Language models can generate text.\n",
        "\"\"\"\n",
        "\n",
        "# Step 2: Tokenize the corpus\n",
        "# Convert the text into a list of individual words (tokens).\n",
        "# Tokenization ensures that punctuation and capitalization are handled properly.\n",
        "tokens = word_tokenize(corpus.lower())  # Convert to lowercase for consistency\n",
        "print(\"Tokens in the corpus:\", tokens)\n",
        "\n",
        "# Step 3: Count bigrams\n",
        "# Create a nested dictionary where bigram_counts[word1][word2] = frequency.\n",
        "# This means we count how many times a specific word is followed by another word.\n",
        "bigram_counts = defaultdict(Counter)\n",
        "for i in range(len(tokens) - 1):  # Loop through tokens, stopping before the last one\n",
        "    first_word = tokens[i]\n",
        "    second_word = tokens[i + 1]\n",
        "    bigram_counts[first_word][second_word] += 1  # Increment the count for the bigram\n",
        "\n",
        "# Step 4: Inspect bigram counts\n",
        "# Print the bigrams that follow the word \"i\" as an example.\n",
        "print(\"\\nExample bigram counts for 'i':\", bigram_counts[\"i\"])\n",
        "\n",
        "# Step 5: Define a function to compute the next word distribution\n",
        "def next_word_distribution(current_word):\n",
        "    \"\"\"\n",
        "    Given the current_word, return a list of (next_word, probability) tuples.\n",
        "    Probabilities are computed by normalizing the raw frequency counts.\n",
        "    \"\"\"\n",
        "    next_word_counts = bigram_counts[current_word]  # Get the counts of words following current_word\n",
        "    total_count = sum(next_word_counts.values())  # Total occurrences of the current_word\n",
        "    distribution = []\n",
        "    for word, count in next_word_counts.items():\n",
        "        # Calculate the probability of each next_word\n",
        "        distribution.append((word, count / total_count))\n",
        "    return distribution\n",
        "\n",
        "# Step 6: Generate text using the bigram model\n",
        "# Start with a random word from the corpus and generate text by sampling from bigram probabilities.\n",
        "current_word = random.choice(list(bigram_counts.keys()))  # Pick a random starting word\n",
        "generated = [current_word]  # Initialize the generated text with the starting word\n",
        "for _ in range(10):  # Generate 10 additional words\n",
        "    dist = next_word_distribution(current_word)  # Get the distribution of next words\n",
        "    if not dist:  # If there are no following words, stop generation\n",
        "        break\n",
        "    words, probs = zip(*dist)  # Separate the words and their probabilities\n",
        "    chosen = random.choices(words, weights=probs, k=1)[0]  # Sample the next word based on probabilities\n",
        "    generated.append(chosen)  # Add the chosen word to the generated text\n",
        "    current_word = chosen  # Update the current word\n",
        "\n",
        "# Step 7: Display the generated text\n",
        "# Combine the generated words into a sentence and print it.\n",
        "print(\"\\nGenerated Text (Bigram Model):\")\n",
        "print(\" \".join(generated))\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMr8sRAkgsBI"
      },
      "source": [
        "## 3. Named Entity Recognition (NER) / Classification:\n",
        "\n",
        "What is Named Entity Recognition (NER)?\n",
        "- Named Entity Recognition (NER) is a subtask of natural language processing (NLP) that identifies and classifies named entities in text into predefined categories.\n",
        "- Examples of named entities include:\n",
        "  - **Person**: Names of individuals (e.g., \"Albert Einstein\").\n",
        "  - **Organization**: Companies, institutions, or groups (e.g., \"Google\", \"UN\").\n",
        "  - **Location**: Geographic locations (e.g., \"Paris\", \"Mount Everest\").\n",
        "  - **Date/Time**: Temporal expressions (e.g., \"January 1, 2024\", \"3 PM\").\n",
        "  - **Others**: Product names, monetary values, percentages, etc.\n",
        "\n",
        "Why Use NER?\n",
        "- **Information Extraction**: Automatically extract structured information from unstructured text.\n",
        "- **Question Answering**: Identify relevant entities in a document to answer specific queries.\n",
        "- **Data Enrichment**: Annotate and link entities to external knowledge bases (e.g., Wikipedia).\n",
        "- **Content Analysis**: Analyze trends or patterns involving specific entities in large text corpora.\n",
        "\n",
        "How Does NER Work?\n",
        "1. **Text Input**: The raw text to be analyzed.\n",
        "2. **Tokenization**: Break down the text into individual tokens (words or subwords).\n",
        "3. **Entity Recognition**: Use a pre-trained NER model or custom-trained model to identify entities.\n",
        "4. **Entity Classification**: Assign each entity to one of the predefined categories.\n",
        "\n",
        "Tools for NER\n",
        "- **spaCy**: A popular NLP library that provides pre-trained models for NER.\n",
        "- **Hugging Face Transformers**: Provides state-of-the-art NER models like BERT and RoBERTa.\n",
        "- **Custom Models**: Train your own NER models using labeled data.\n",
        "\n",
        "Usefulness of NER\n",
        "- **Real-Time Applications**: Powering chatbots, virtual assistants, and recommendation systems.\n",
        "- **Business Intelligence**: Extracting actionable insights from text (e.g., contracts, news).\n",
        "- **Healthcare**: Identifying medical terms, drug names, or patient details in clinical notes.\n",
        "- **Finance**: Analyzing financial documents for entities like companies and stock symbols.\n",
        "\n",
        "Limitations of NER\n",
        "1. **Domain Dependence**:\n",
        "   - Pre-trained models may not perform well in specialized domains (e.g., legal, biomedical).\n",
        "   - Requires domain-specific training data for customization.\n",
        "2. **Ambiguity**:\n",
        "   - Entities can have multiple meanings (e.g., \"Apple\" as a company vs. fruit).\n",
        "   - Context is critical for disambiguation, which can be challenging for models.\n",
        "3. **Language Limitations**:\n",
        "   - Models trained on one language may not generalize well to others without additional training.\n",
        "4. **Scalability**:\n",
        "   - Processing large datasets in real time may require significant computational resources.\n",
        "\n",
        "When to Use NER?\n",
        "- To identify key entities in large unstructured datasets for downstream analysis.\n",
        "- When building systems that rely on entity-specific logic, such as search engines or recommendation systems.\n",
        "- In tasks requiring structured data extraction from text, like contract analysis or customer feedback processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQrCZGbtBrdB",
        "outputId": "4c5a1084-1d25-4ba0-9a25-042e9cf0f14a"
      },
      "outputs": [],
      "source": [
        "# if you are using colab, uncomment the following line\n",
        "# Install and download spaCy\n",
        "# !pip install -qqq spacy  # Install the spaCy library\n",
        "!python -m spacy download en_core_web_sm  # Download the small English language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "evMPGFWwQw3l",
        "outputId": "729929ba-21a2-4696-bf37-5086107b2d3a"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import spacy\n",
        "import random\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the spaCy language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Load the small English model for NLP tasks\n",
        "\n",
        "# Step 1: Create a list of sample texts\n",
        "texts = [\n",
        "    \"Barack Obama was the 44th President of the United States.\",\n",
        "    \"Apple Inc. is headquartered in Cupertino, California.\",\n",
        "    \"The Eiffel Tower is a famous landmark in Paris, France.\",\n",
        "    \"Marie Curie was a pioneering scientist in the field of radioactivity.\",\n",
        "    \"The Amazon rainforest is the largest tropical rainforest in the world.\"\n",
        "]\n",
        "\n",
        "# Function to process text with a simulated delay\n",
        "def process_text_with_delay(text):\n",
        "    \"\"\"\n",
        "    Processes a given text with spaCy to extract named entities.\n",
        "    Adds a small random delay to simulate longer processing times.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to process.\n",
        "\n",
        "    Returns:\n",
        "        doc: spaCy processed Doc object containing parsed text and entities.\n",
        "    \"\"\"\n",
        "    doc = nlp(text)  # Process the text with the spaCy model\n",
        "    time.sleep(random.uniform(0.5, 2))  # Introduce a delay (0.5 to 2 seconds) for simulation\n",
        "    return doc\n",
        "\n",
        "# Step 2: Process each text and extract named entities\n",
        "all_entities = []  # List to store all extracted entities across texts\n",
        "for text in texts:\n",
        "    doc = process_text_with_delay(text)  # Process the text\n",
        "    # Extract entities as tuples of (text, label, explanation)\n",
        "    entities = [(ent.text, ent.label_, spacy.explain(ent.label_)) for ent in doc.ents]\n",
        "    all_entities.extend(entities)  # Add entities to the global list\n",
        "\n",
        "    # Print extracted entities for the current text\n",
        "    print(f\"Entities in '{text}':\")\n",
        "    for ent_text, ent_label, ent_explanation in entities:\n",
        "        print(f\"  - {ent_text} ({ent_label}): {ent_explanation}\")\n",
        "    print(\"-\" * 20)  # Separator for better readability\n",
        "\n",
        "# Step 3: Analyze entity frequencies\n",
        "entity_frequencies = {}  # Dictionary to store frequencies of entities by type\n",
        "for ent_text, ent_label, _ in all_entities:\n",
        "    # Update the frequency of each entity under its label\n",
        "    if ent_label not in entity_frequencies:\n",
        "        entity_frequencies[ent_label] = {}\n",
        "    if ent_text in entity_frequencies[ent_label]:\n",
        "        entity_frequencies[ent_label][ent_text] += 1\n",
        "    else:\n",
        "        entity_frequencies[ent_label][ent_text] = 1\n",
        "\n",
        "# Print a summary of entity frequencies\n",
        "print(\"\\nEntity Frequencies:\")\n",
        "for ent_label, entities in entity_frequencies.items():\n",
        "    print(f\"  {ent_label}:\")\n",
        "    for ent_text, frequency in entities.items():\n",
        "        print(f\"    - {ent_text}: {frequency}\")\n",
        "\n",
        "# Step 4: Visualize entity type frequencies using a bar chart\n",
        "entity_labels = list(entity_frequencies.keys())  # Extract entity labels\n",
        "entity_counts = [len(entities) for entities in entity_frequencies.values()]  # Count unique entities per label\n",
        "\n",
        "plt.figure(figsize=(10, 5))  # Set figure size\n",
        "plt.bar(entity_labels, entity_counts, color='skyblue')  # Create a bar chart\n",
        "plt.title('Frequency of Entity Types')  # Add a title to the chart\n",
        "plt.xlabel('Entity Type')  # Label for the x-axis\n",
        "plt.ylabel('Frequency')  # Label for the y-axis\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
        "plt.tight_layout()  # Adjust layout to prevent overlapping labels\n",
        "plt.show()  # Display the chart"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "GenAI",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
