{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRXXroccDA6F"
      },
      "source": [
        "# 3.2 Tokens and Tokenization\n",
        "\n",
        "## Attribution\n",
        "This notebook was re-used and modified from material created by NVIDIA and Dartmouth College and licensed under the Creative Commons Attribution-Non Commercial 4.0 International License (CC BY-NC 4.0) for the **Generative AI: Theory and Applications** MSc Module at UWS.\n",
        "Source materials available at: https://developer.nvidia.com/gen-ai-teaching-kit-syllabus (NVIDIA Deep Learning Institute Generative AI Teaching Kit) \n",
        "\n",
        "## Overview\n",
        "\n",
        "Welcome to the second notebook in this week. In this notebook, we will dive deeper into key Natural Language Processing (NLP) tasks and methods that are foundational for processing and analyzing text data.\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "- Explore basic tokenization methods, including word, character, and sentence-level tokenization.\n",
        "- Utilize `tiktoken` to tokenize text for GPT-like models, analyzing how different tokenizers handle text.\n",
        "- Train and evaluate a subword tokenizer using Byte Pair Encoding (BPE) for efficient text representation.\n",
        "\n",
        "These tasks will help you build a strong understanding of tokenization and its applications, setting the stage for advanced NLP workflows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FOAR6qzFXiu"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Javascript\n",
        "display(Javascript('IPython.notebook.kernel.restart();'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_BpbbflDAtA"
      },
      "source": [
        "## 1. Basic Tokenization\n",
        "\n",
        "What is Tokenization?\n",
        "- Tokenization is the process of splitting text into smaller units, such as words, characters, or sentences.\n",
        "- It serves as the foundation for almost every NLP task by breaking raw text into manageable components.\n",
        "\n",
        "Why is Tokenization Important?\n",
        "- Enables systematic processing of text data.\n",
        "- Breaks sentences into units, allowing for analysis like word frequencies or semantic meaning.\n",
        "- Prepares text for machine learning models, ensuring uniformity in input.\n",
        "\n",
        "What This Section Covers:\n",
        "1. **Word-level Tokenization**: Breaking text into individual words for analysis.\n",
        "2. **Character-level Tokenization**: Splitting text into individual characters.\n",
        "3. **Sentence-level Tokenization**: Dividing text into complete sentences.\n",
        "\n",
        "Tools Used:\n",
        "- **NLTK (Natural Language Toolkit)**: A popular Python library for natural language processing.\n",
        "- Provides efficient methods for tokenization at word, character, and sentence levels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e24iIB0yF1Uv"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('punkt_tab', quiet=True)  # Ensure required data is downloaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrM8_3WRDY59"
      },
      "outputs": [],
      "source": [
        "# ### Introduction to Basic Tokenization\n",
        "# - Tokenization is the process of splitting text into smaller units like words, characters, or sentences.\n",
        "# - This section demonstrates:\n",
        "#   1. Word-level tokenization: Breaking text into individual words.\n",
        "#   2. Character-level tokenization: Breaking text into individual characters.\n",
        "#   3. Sentence-level tokenization: Splitting text into sentences.\n",
        "# - We'll use NLTK for basic tokenization tasks to show its functionality.\n",
        "\n",
        "\"\"\"\n",
        "Demonstrates tokenization at word, character, and sentence levels.\n",
        "\"\"\"\n",
        "# === Basic Tokenization Examples ===\n",
        "text = \"Hello world! This is a short sentence. Here's another one.\"\n",
        "# Text to be tokenized\n",
        "\n",
        "# Word-level tokenization using NLTK\n",
        "words = word_tokenize(text)\n",
        "# Breaks the text into individual words, accounting for punctuation\n",
        "\n",
        "# Character-level tokenization\n",
        "chars = list(text)\n",
        "# Splits the text into individual characters, including spaces and punctuation\n",
        "\n",
        "# Sentence-level tokenization using NLTK\n",
        "sentences = sent_tokenize(text)\n",
        "# Splits the text into complete sentences based on punctuation and grammar rules\n",
        "\n",
        "# Print outputs for each tokenization level\n",
        "print(\"Original Text:\\n\", text)\n",
        "print(\"\\nWord Tokenization (NLTK):\", words)\n",
        "print(\"\\nCharacter Tokenization:\", chars)\n",
        "print(\"\\nSentence Tokenization (NLTK):\", sentences)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BSxnGP2Dnta"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiAfZ3I_DHPP"
      },
      "source": [
        "## 2. Tokenization with Tiktoken\n",
        "\n",
        "What is tiktoken?\n",
        "- `tiktoken` is OpenAIâ€™s library for efficient tokenization of text.\n",
        "- It is designed specifically for tokenizing input and decoding outputs for GPT models.\n",
        "\n",
        "Why Use tiktoken?\n",
        "- Provides tokenizer compatibility with different GPT-like models (e.g., GPT-2, GPT-3, GPT-4).\n",
        "- Efficiently encodes text into token IDs and decodes token IDs back into text.\n",
        "- Allows visualization and analysis of tokenization for various OpenAI models.\n",
        "\n",
        "What This Section Covers:\n",
        "1. Demonstrates tokenization using `tiktoken` for different models (e.g., GPT-2 and GPT-3.5).\n",
        "2. Shows how tokenization differs across models.\n",
        "3. Explores edge cases, such as handling special tokens and long texts.\n",
        "\n",
        "Applications of tiktoken:\n",
        "- Preprocessing text for OpenAI API inputs to ensure token limits are respected.\n",
        "- Analyzing tokenization for optimal prompt design and cost estimation.\n",
        "\n",
        "Tools Used:\n",
        "- **tiktoken**: OpenAI's tokenization library for GPT models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAU7MvjsDWsF"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "\"\"\"\n",
        "Demonstrates tokenization and decoding using tiktoken for different models.\n",
        "\"\"\"\n",
        "# === tiktoken Examples ===\n",
        "print(\"=== tiktoken Tokenization Examples ===\")\n",
        "\n",
        "# Step 1: Define a sample text\n",
        "sample_text = (\n",
        "    \"Hello, how are you today? Let's explore tokenization with tiktoken! \"\n",
        "    \"It supports GPT-2, GPT-3, and GPT-4 models.\"\n",
        ")\n",
        "print(\"\\nSample Text:\\n\", sample_text)\n",
        "\n",
        "# Step 2: Load encoders for different models\n",
        "encoders = {\n",
        "    \"GPT-2\": tiktoken.get_encoding(\"gpt2\"),\n",
        "    \"GPT-3.5 Turbo\": tiktoken.encoding_for_model(\"gpt-3.5-turbo\"),\n",
        "    \"GPT-4\": tiktoken.encoding_for_model(\"gpt-4\"),\n",
        "}\n",
        "\n",
        "# Step 3: Tokenize the text with each model's encoder\n",
        "for model, encoder in encoders.items():\n",
        "    print(f\"\\n--- {model} Tokenization ---\")\n",
        "\n",
        "    # Encode the text to token IDs\n",
        "    token_ids = encoder.encode(sample_text)\n",
        "    print(\"Encoded Token IDs:\", token_ids)\n",
        "\n",
        "    # Decode back to text\n",
        "    decoded_text = encoder.decode(token_ids)\n",
        "    print(\"Decoded Text:\", decoded_text)\n",
        "\n",
        "    # Token-level decoding (optional for visualization)\n",
        "    individual_tokens = [encoder.decode([tid]) for tid in token_ids]\n",
        "    print(\"Individual Decoded Tokens:\", individual_tokens)\n",
        "\n",
        "    # Length of the tokenized text\n",
        "    print(\"Number of Tokens:\", len(token_ids))\n",
        "\n",
        "# Step 4: Explore long text edge cases\n",
        "long_text = \"This is a demonstration of how tiktoken handles very long inputs. \" * 50\n",
        "encoder = encoders[\"GPT-3.5 Turbo\"]\n",
        "long_token_ids = encoder.encode(long_text)\n",
        "print(\"\\n--- Long Text Tokenization (GPT-3.5 Turbo) ---\")\n",
        "print(\"Number of Tokens in Long Text:\", len(long_token_ids))\n",
        "\n",
        "# Step 5: Analyze special tokens\n",
        "print(\"\\n--- Special Tokens ---\")\n",
        "print(\"GPT-3.5 Turbo Special Tokens:\")\n",
        "\n",
        "# Corrected special token handling\n",
        "special_tokens = encoder._special_tokens  # Access the special tokens dictionary\n",
        "for token_name, token_value in special_tokens.items():\n",
        "    print(f\"  {token_name}: {token_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AJiReCIE0HD"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HwDbQrNDLUJ"
      },
      "source": [
        "## 3. Training and using a BytePair Encoding Tokenizer\n",
        "\n",
        "What is Subword Tokenization?\n",
        "- Subword tokenization is an intermediate approach between word-level and character-level tokenization.\n",
        "- It breaks words into smaller units (subwords) to handle out-of-vocabulary (OOV) words and reduce vocabulary size.\n",
        "\n",
        "Why is Subword Tokenization Important?\n",
        "- Handles rare or unknown words by breaking them into known subwords.\n",
        "- Reduces vocabulary size, making models more efficient.\n",
        "- Retains semantic meaning by representing words as a sequence of subwords.\n",
        "\n",
        "What This Section Covers:\n",
        "1. Training a Subword Tokenizer: Demonstrates Byte Pair Encoding (BPE) on a small custom corpus.\n",
        "2. Using the Tokenizer: Tests the trained tokenizer on example sentences, including OOV words.\n",
        "3. Fine-tuning the Tokenizer: Adds new words or phrases to the vocabulary through incremental training.\n",
        "\n",
        "Applications of Subword Tokenization:\n",
        "- Pretraining large language models like BERT or GPT, which rely on subword-level tokenization.\n",
        "- Handling morphologically rich languages with complex word structures.\n",
        "\n",
        "Tools Used:\n",
        "- **Hugging Face Tokenizers Library**: Provides the tools to build and train BPE tokenizers.\n",
        "- Includes components like pre-tokenizers, decoders, and trainers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63VFyBTMDVJ3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tokenizers import Tokenizer, trainers, models, pre_tokenizers, decoders\n",
        "\n",
        "# Step 1: Define a small corpus for training\n",
        "corpus = [\n",
        "    \"I love natural language processing.\",\n",
        "    \"Tokenization is an essential step in NLP.\",\n",
        "    \"Subword tokenization helps handle out-of-vocabulary words.\",\n",
        "    \"Byte Pair Encoding can reduce the vocabulary size significantly.\",\n",
        "    \"This is a demonstration of building our own BPE tokenizer.\",\n",
        "    \"We'll later fine-tune this tokenizer on new data.\"\n",
        "]\n",
        "\n",
        "# Step 2: Initialize and configure the tokenizer\n",
        "tokenizer = Tokenizer(models.BPE())\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "tokenizer.decoder = decoders.BPEDecoder()\n",
        "\n",
        "# Step 3: Configure the BPE trainer\n",
        "trainer = trainers.BpeTrainer(\n",
        "    vocab_size=100,  # Define a small vocabulary for demonstration\n",
        "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]  # Add special tokens\n",
        ")\n",
        "\n",
        "# Step 4: Write the corpus to a temporary file for training\n",
        "temp_filename = \"temp_bpe_corpus.txt\"\n",
        "with open(temp_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in corpus:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "# Step 5: Train the tokenizer on the corpus\n",
        "tokenizer.train([temp_filename], trainer)\n",
        "\n",
        "# Step 6: Cleanup temporary file\n",
        "if os.path.exists(temp_filename):\n",
        "    os.remove(temp_filename)\n",
        "\n",
        "# Output results\n",
        "print(\"Tokenizer training complete!\")\n",
        "print(\"Vocabulary Size:\", tokenizer.get_vocab_size())\n",
        "\n",
        "# Step 7: Explore the vocabulary\n",
        "vocab = tokenizer.get_vocab()\n",
        "print(\"\\nSample Vocabulary:\")\n",
        "for token, idx in list(vocab.items())[:10]:  # Show the first 10 tokens\n",
        "    print(f\"  {token}: {idx}\")\n",
        "\n",
        "# Step 8: Test the trained tokenizer on new sentences\n",
        "test_sentences = [\n",
        "    \"I enjoy natural language processing!\",\n",
        "    \"Out-of-vocabulary words like SubwordifyMe are handled.\",\n",
        "    \"Byte Pair Encoding is efficient for reducing vocabulary.\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    encoded = tokenizer.encode(sentence)\n",
        "    print(\"\\nOriginal Sentence:\", sentence)\n",
        "    print(\"Encoded Tokens:\", encoded.tokens)\n",
        "    print(\"Token IDs:\", encoded.ids)\n",
        "    print(\"Decoded Sentence:\", tokenizer.decode(encoded.ids))\n",
        "\n",
        "# Step 9: Visualize subword splitting for OOV words\n",
        "oov_word = \"SubwordifyMePlease\"\n",
        "encoded_oov = tokenizer.encode(oov_word)\n",
        "print(\"\\nOOV Word:\", oov_word)\n",
        "print(\"Encoded Tokens:\", encoded_oov.tokens)\n",
        "print(\"Decoded OOV Word:\", tokenizer.decode(encoded_oov.ids))\n",
        "\n",
        "# Step 10: Fine-tune the tokenizer with new data\n",
        "additional_corpus = [\n",
        "    \"Fine-tuning a tokenizer can adapt it to new domains.\",\n",
        "    \"Specialized vocabularies can be added incrementally.\",\n",
        "    \"Let's add new data for incremental training!\"\n",
        "]\n",
        "\n",
        "# Write the additional corpus to a temporary file\n",
        "temp_filename_additional = \"temp_additional_corpus.txt\"\n",
        "with open(temp_filename_additional, \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in additional_corpus:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "# Fine-tune the tokenizer\n",
        "tokenizer.train([temp_filename_additional], trainer)\n",
        "\n",
        "# Cleanup temporary file\n",
        "if os.path.exists(temp_filename_additional):\n",
        "    os.remove(temp_filename_additional)\n",
        "\n",
        "# Output fine-tuning results\n",
        "print(\"\\nTokenizer fine-tuned with additional data!\")\n",
        "print(\"New Vocabulary Size:\", tokenizer.get_vocab_size())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "GenAI",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
